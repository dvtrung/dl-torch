model: dl_torch.models.attention.NMT
dataset:
  name: src.datasets.nmt.Tatoeba
batch_size: 128
num_epochs: 50
optimizer:
  name: adam
  lr: 0.01
loss: nll_loss.SeqNLLLoss
output_format: text
max_length: 10
teacher_forcing_ratio: 0.0
encoder:
  rnn_type: lstm
  bidirectional: false
  num_layers: 1
  hidden_size: 256
  embedding: none
  update_embedding: true
decoder:
  rnn_type: lstm
  use_attention: true
  num_layers: 1
  hidden_size: 256
  max_length: 10
dropout_p: 0.1
metrics:
  - bleu
