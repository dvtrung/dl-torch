backend: pytorch
model:
  name: dlex.torch.models.attention.Attention
  encoder:
    rnn_type: lstm
    bidirectional: false
    num_layers: 5
    input_size: 256
    hidden_size: 512
    output_size: 256
  decoder:
    rnn_type: lstm
    use_attention: true
    num_layers: 1
    hidden_size: 256
    output_size: 256
    max_length: 20
  beam_search:
    beam_size: 16
    penalty: 0.5
  attention:
    type: bahdanau
    size: 256
  decoding_method: greedy
  dropout: 0.2
  teacher_forcing_ratio: 0.5
dataset:
  name: dlex.datasets.voice.common_voice.common_voice.CommonVoiceBuilder
  alias: common_voice
  unit: word
  output_format: "text"
  debug_size: 100
  sort: true
  max_target_length: 25
  max_source_length: 2000
  feature:
    tool: htk
    file_type: htk
    num_filters: 120
  special_tokens: [sos, eos, oov, pad]
train:
  batch_size:
    0: 50
    10: 40
    20: 32
    40: 20
    75: 16
    90: 8
  num_epochs: 100
  optimizer:
    name: adam
    lr: 0.001
  max_grad_norm: 5.0
  save_every: 0.05e
  log_every: 60s
test:
  batch_size: 2
  metrics:
    - wer
