model:
  name: dlex.models.attention.Attention
  encoder:
    rnn_type: lstm
    bidirectional: false
    num_layers: 3
    embedding_size: 256
    hidden_size: 512
    output_size: 256
  decoder:
    rnn_type: lstm
    use_attention: true
    num_layers: 1
    hidden_size: 256
    output_size: 256
    max_length: 20
  beam_search:
    beam_size: 16
    penalty:
  attention:
    type: location
    size: 256
    num_channels: 8
    filter_size: 64
  decoding_method: greedy
  dropout: 0.2
  teacher_forcing_ratio: 0.5
dataset:
  name: src.datasets.vivos.VIVOS
  unit: word
  output_format: "text"
  debug_size: 100
  sort: true
batch_size: 16
num_epochs: 300
optimizer:
  name: adam
  lr: 0.001
max_grad_norm: 5.0
metrics:
  - wer
