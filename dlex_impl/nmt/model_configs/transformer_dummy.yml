backend: pytorch
model:
  name: dlex.torch.models.transformer.Transformer
  num_heads: 1
  key_size: 32
  value_size: 32
  dim_model: 64
  dim_inner: 64
  encoder:
    rnn_type: lstm
    bidirectional: false
    num_layers: 3
    input_size: 50
    hidden_size: 64
    output_size: 64
    embedding: none
    update_embedding: true
    max_length: 50
  decoder:
    share_embeddings: false
    use_attention: true
    num_layers: 3
    hidden_size: 64
    max_length: 50
    output_size: 64
  beam_search:
    beam_size: 16
    penalty: 0
  attention:
    type: bahdanau
    size: 256
  dropout: 0.1
  teacher_forcing_ratio: 1.0
  decoding_method: greedy
dataset:
  alias: dummy
  source: vi
  target: en
  output_format: text
  debug_size: 20
  unit: word
  max_target_length: 50
  max_source_length: 50
  special_tokens: [sos, eos, oov, pad]
train:
  batch_size: 64
  num_epochs: 1000
  optimizer:
    name: adam
    lr: 0.001
  max_grad_norm: 1.0
test:
  metrics: [wer]
